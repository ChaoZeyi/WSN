***[2017.6.10  晁泽依]()***

距离度量总结：

1. **闵可夫斯基距离**
2. **欧几里得距离**
3. **曼哈顿距离**
4. **切比雪夫距离**
5. **余弦相似度**
6. **皮尔逊相关系数**
7. **汉明距离**
8. **杰卡德相似系数**
9. **DTW 距离**
10. **KL 散度**

## 1. 闵可夫斯基距离

**闵可夫斯基距离**（Minkowski distance）是衡量数值点之间距离的一种非常常见的方法，假设数值点 P 和 Q 坐标如下：

![img](http://images.cnitblog.com/blog/533521/201308/07220422-b6c5a38eccb74824b92ba1b40c9dd92f.png)

那么，闵可夫斯基距离定义为：

![img](http://images.cnitblog.com/blog/533521/201308/07220504-12655edb08dc45ae8a036d8028743042.png)

该距离最常用的 p 是 2 和 1, 前者是**欧几里得距离**（Euclidean distance），后者是**曼哈顿距离**（Manhattan distance）。

当 p 趋近于无穷大时，闵可夫斯基距离转化成**切比雪夫距离**（Chebyshev distance）：

![img](http://images.cnitblog.com/blog/533521/201308/07220549-4fb4c30e7fb84ca290d04f44f75dea7b.png)

闵可夫斯基距离比较直观，但是它与数据的分布无关，具有一定的局限性，如果 x 方向的幅值远远大于 y 方向的值，这个距离公式就会过度放大 x 维度的作用。所以，在计算距离之前，我们可能还需要对数据进行 **归一化**处理，即减去均值，除以标准差。

## 2. 向量内积

向量内积是线性代数里最为常见的计算，实际上它还是一种有效并且直观的相似性测量手段。向量内积的定义如下：

![01.png](https://github.com/ChaoZeyi/WSN/blob/master/photos/01.png?raw=true)

## 3. 余弦相似度

向量内积的结果是没有界限的，一种解决办法是除以长度之后再求内积，这就是应用十分广泛的**余弦相似度**（Cosine similarity）：

![02.png](https://github.com/ChaoZeyi/WSN/blob/master/photos/02.png?raw=true)

## 4. 皮尔逊相关系数

皮尔逊相关系数（Pearson correlation），有时候也直接叫相关系数：

![03.png](https://github.com/ChaoZeyi/WSN/blob/master/photos/03.png?raw=true)

将 x 与 y 对应位置的两个数值看作一个样本点，皮尔逊系数用来表示这些样本点分布的相关性。

该距离对数据类型有要求，x、y必须都是正态分布数据。

## 5. 汉明距离

汉明距离（Hamming distance）是指，两个等长字符串s1与s2之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。举个维基百科上的例子：

![img](http://images.cnitblog.com/blog/533521/201308/07221109-c683a8f31c9a4e31a93e5d04fdab3443.png)

还可以用简单的**匹配系数**来表示两点之间的相似度——匹配字符数/总字符数。

在一些情况下，某些特定的值相等并不能代表什么。举个例子，用 1 表示用户看过该电影，用 0 表示用户没有看过，那么用户看电影的的信息就可用 0,1 表示成一个序列。考虑到电影基数非常庞大，用户看过的电影只占其中非常小的一部分，如果两个用户都没有看过某一部电影（两个都是 0），并不能说明两者相似。反而言之，如果两个用户都看过某一部电影（序列中都是 1），则说明用户有很大的相似度。在这个例子中，序列中等于 1 所占的权重应该远远大于 0 的权重，这就引出下面要说的**杰卡德相似系数**（Jaccard similarity）。

在上面的例子中，用 M11 表示两个用户都看过的电影数目，M10 表示用户 A 看过，用户 B 没看过的电影数目，M01 表示用户 A 没看过，用户 B 看过的电影数目，M00 表示两个用户都没有看过的电影数目。Jaccard 相似性系数可以表示为：

![img](http://images.cnitblog.com/blog/533521/201308/07221130-ef3d75ecd11848c7b6d499bd52271f4c.png)

如果分类数值点是用树形结构来表示的，它们的相似性可以用相同路径的长度来表示，比如，“/product/spot/ballgame/basketball” 离“product/spot/ballgame/soccer/shoes” 的距离小于到 "/product/luxury/handbags" 的距离，以为前者相同父节点路径更长。

## 6. DTW 距离

**DTW 距离**（Dynamic Time Warp）是指序列在时间上不匹配的时候一种衡量相似度的方法。举个例子，两份原本一样声音样本A、B都说了“你好”，A在时间上发生了扭曲，“你”这个音延长了几秒。最后A:“你~~~~~~~好”，B：“你好”。DTW正是这样一种可以用来匹配A、B之间的最短距离的算法。

DTW 距离在保持信号先后顺序的限制下对时间信号进行“膨胀”或者“收缩”，找到最优的匹配，是一个动态规划的问题。

## 7. KL 散度

前面我们谈论的都是两个数值点之间的距离，实际上两个样本分布之间的距离是可以测量的。在统计学里面经常需要测量两组样本分布之间的距离，进而判断出它们是否出自同一个 population，常见的方法：**KL 散度**（ KL-Divergence）

KL 散度又叫**相对熵**（relative entropy），设 ***p(x)*** 和 ***q(x)*** 是![img](http://images.cnitblog.com/blog/571227/201501/072027554214250.png)取值的两个概率概率分布，则![img](http://images.cnitblog.com/blog/571227/201501/072028528906593.png)对![img](http://images.cnitblog.com/blog/571227/201501/072029217345115.png)的相对熵为:

​                                                                   ![img](http://images.cnitblog.com/blog/571227/201501/072030106714463.png)

在一定程度上，可以用来度量两个随机分布的距离，当两个随机分布相同时，它们的相对熵为零，当两个随机分布的差别增大时，它们的相对熵也会增大。KL散度是两个随机分布P和Q差别的非对称性的度量。 典型情况下，P表示数据的真实分布，Q 表示数据的理论分布，模型分布，或P的近似分布。

